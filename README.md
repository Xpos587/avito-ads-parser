# Avito Ads Parser

Пайплайн для поиска пробелов в покрытии товарного каталога путём анализа объявлений на маркетплейсе.

## Зачем это нужно

Компания продаёт запчасти для тяжёлой техники и хочет системно находить, какие комбинации товаров отсутствуют в каналах продаж. Пайплайн берёт сырые объявления с маркетплейса, обогащает их классификацией товаров и сравнивает с целевым каталогом, чтобы найти пробелы в покрытии.

## Как это работает

Данные проходят через три этапа, каждый с одной ответственностью:

1. **Extract** (`src/parser.py`) — Извлекает объявления из HTML-снимков
2. **Enrich** (`src/enricher.py`) — Классифицирует объявления через внешний API
3. **Analyze** (`src/analyzer.py`) — Находит разницу между тем, что есть, и тем, что нужно

Каждый этап изолирован и может работать независимо. Если нужно добавить новый источник данных (например, парсить другой маркетплейс), добавьте парсер в `src/` и подключите его в `main.py`. Если нужен другой API для обогащения — замените enricher. Analyzor работает только с финальным обогащённым форматом.

## Структура проекта

```
avito-ads-parser/
├── src/          # Основные модули пайплайна (parser → enricher → analyzer)
├── data/         # Входные (HTML, каталог) и выходные (CSV) данные
├── logs/         # Логи API-запросов для отладки
├── tests/        # 97% покрытие тестами
└── main.py       # Оркестрация полного пайплайна
```

## Формат данных

Все модули работают с одинаковой структурой объявления. Парсер извлекает `ad_id`, `title`, `url`, `region`, `price`. Enricher добавляет `group0-5` (иерархия товаров), `marka`, `model`. Analyzer сравнивает комбинации `group0 + group1 + group2` с целевым каталогом.

Если нужно отслеживать дополнительные поля, расширьте dataclass `Ad` в `src/parser.py` и обновите колонки групп в analyzer.

## Быстрый старт

```bash
# Установка зависимостей (Python 3.14+)
uv sync

# Настройка API-ключа
cp .env.example .env
# Отредактируйте .env и добавьте: TOP505_API_KEY=ваш_ключ

# Запуск полного пайплайна
uv run python main.py
```

## Результат работы

В `data/` генерируются три файла:

- `ads_raw.csv` — Сырые объявления, извлечённые из HTML
- `ads_enriched.csv` — Объявления с классификацией товаров от API
- `missing_coverage.csv` — Комбинации товаров, отсутствующие в объявлениях, отсортированные по приоритету

## Разработка

```bash
# Линтинг, типизация, поиск мёртвого кода
uv run check

# Запуск тестов
uv run test

# Тесты с покрытием
uv run test-cov
```

### Добавление парсера нового маркетплейса

Создайте новый модуль парсера в `src/`, который возвращает ту же структуру `Ad`, затем импортируйте его в `main.py` и замените `parse_html_files()`.

### Смена API обогащения

Измените `src/enricher.py` для вызова вашего API. Остальная часть пайплайна ожидает поля `group0-5`, `marka`, `model` в обогащённом выводе.

### Настройка анализа покрытия

Отредактируйте список `group_cols` в `src/analyzer.py`, если хотите сравнивать по другим полям.

## Результаты на реальных данных

На реальных данных (100 объявлений с Авито):
- **Извлечение:** 100 объявлений распарсено
- **Обогащение:** 90/100 (90% успех)
- **Покрытие:** 6/115 комбинаций (5.22%)
- **Найдено пробелов:** 109 отсутствующих комбинаций

## Конфигурация через переменные окружения

| Переменная | Назначение | Обязательно |
|------------|------------|-------------|
| `TOP505_API_KEY` | API-ключ для сервиса обогащения top505.ru | Да |

Установите их в файле `.env` (шаблон в `.env.example`).

---

# Детали реализации (по требованиям ТЗ)

## Как парсились HTML-страницы

Используется `BeautifulSoup` с парсером `lxml` для извлечения объявлений из HTML-снимков Авито.

**Извлекаемые поля:**
| Поле | Источник | Обязательное |
|------|----------|--------------|
| `ad_id` | атрибут `data-item-id` | Да |
| `title` | тег `data-marker="item-title"` | Да |
| `url` | атрибут `href` ссылки | Нет |
| `region` | тег `data-marker="item-location"` | Нет |
| `price` | тег `data-marker="item-price"` | Нет |

Ограничения соблюдены: нет обращений к интернету, нет авторизации/прокси, используется только локальный HTML.

## Пример запроса и ответа к API

**Запрос:**
```http
POST https://top505.ru/api/item_batch
Content-Type: application/json
X-API-Key: <ваш-API-ключ>

{
  "source": "1c",
  "data": [
    {
      "title": "Натяжитель гусеницы CAT 312 / 188-0895",
      "day": "2026-01-22"
    }
  ]
}
```

**Ответ (HTTP 200):**
```json
{
  "processed_data": [
    {
      "title": "Натяжитель гусеницы CAT 312 / 188-0895",
      "raw_item": "Натяжитель гусеницы CAT 312 / 188-0895",
      "day": "2026-01-22",
      "marka": "cat",
      "model": "312",
      "catalog_number": "188-0895",
      "group0": "ходовая часть",
      "group1": "натяжитель",
      "group2": "натяжитель в сборе",
      "group3": null,
      "group4": null,
      "clear_item": "натяжитель гусеницы cat 312 / 188-0895"
    }
  ]
}
```

## Обработка ошибок и лимитов

| Код | Обработка |
|-----|-----------|
| **200** | Успех — данные сохраняются |
| **401** | Ошибка авторизации — логируется, выбрасывается `AuthError` |
| **429** | Rate limit — экспоненциальная пауза (2^attempt секунд) + retry до 3 раз |
| **5xx** | Временная ошибка сервера — retry до 3 раз с паузой 1 секунда |
| **Timeout** | Таймаут — retry до 3 раз |
| **Не-JSON** | Логирование ошибки + пропуск записи |

**Лимиты:**
- Скорость: 2-5 req/s (реализовано через `asyncio.sleep(0.5)` между батчами)
- Размер батча: до 200 объектов
- Таймаут: 30 секунд

**Логирование:** подробная статистика в `logs/api_log.txt` (сколько отправлено, успешно, % успеха, типы ошибок, количество retry).

---

# Что можно сделать дальше (1-2 недели)

1. **CLI интерфейс** — `argparse` для гибкости: выбор входных файлов, настройка batch size, пороговое значение для покрытия
2. **Конфигурационный файл** — `config.yaml` для настроек API, путей к файлам, параметров логирования
3. **Кеширование** — SQLite/redis для хранения результатов обогащения и предотвращения повторных API вызовов
4. **Дополнительные метрики** — статистика по брендам, категориям, динамика покрытия во времени
5. **Прогресс-бар** — `tqdm` для визуализации прогресса при обработке больших объёмов
6. **Параллелизация** — одновременный парсинг множества HTML-файлов с `asyncio.gather`
7. **Валидация данных** — `pydantic` модели для валидации входных/выходных данных
8. **Docker-контейнер** — для упрощения деплоя и воспроизводимости окружения
9. **Airflow/Prefect DAG** — оркестрация пайплайна с периодическим запуском
10. **Unit-тесты для edge cases** — больше тестов для граничных случаев (пустые файлы, невалидный HTML, ошибки сети)
